{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\ML LABS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from timeseires.utils.to_split import to_split\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from timeseires.utils.multivariate_single_step import multivariate_single_step\n",
    "from timeseires.utils.univariate_multi_step import univariate_multi_step\n",
    "from timeseires.utils.univariate_single_step import univariate_single_step\n",
    "from timeseires.utils.CosineAnnealingLRS import CosineAnnealingLRS\n",
    "from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Add\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookback = 24\n",
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(time_steps , num_features)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">504</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m504\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m16,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,193</span> (63.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,193\u001b[0m (63.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,193</span> (63.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,193\u001b[0m (63.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = MLP()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "tensorflow.keras.utils.plot_model(model1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = r\"D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\"\n",
    "OUTPUT_PATH = r'D:\\ML LAB7'\n",
    "#FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"\\history.png\"])\n",
    "#JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"\\history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "#TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model =MLP()\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84907, 21), (24259, 21), (12130, 21))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path_dataset =r'D:\\ML LABS\\lab 8,9,10\\Dataset'\n",
    "path_tr = os.path.join(path_dataset, 'AEP_train.csv')\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values\n",
    "path_v = os.path.join(path_dataset, 'AEP_validation.csv')\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "path_te = os.path.join(path_dataset, 'AEP_test.csv')\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "\n",
    "path_scaler = os.path.join(path_dataset, 'AEP_Scaler.pkl')\n",
    "scaler         = pickle.load(open(path_scaler, 'rb'))\n",
    "\n",
    "train_set.shape, validation_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aep</th>\n",
       "      <th>Is_holiday1</th>\n",
       "      <th>Is_holiday2</th>\n",
       "      <th>Is_Weekend1</th>\n",
       "      <th>Is_Weekend2</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_week</th>\n",
       "      <th>cos_week</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>...</th>\n",
       "      <th>sin_wintert</th>\n",
       "      <th>cos_wintert</th>\n",
       "      <th>sin_springt</th>\n",
       "      <th>cos_springt</th>\n",
       "      <th>sin_summert</th>\n",
       "      <th>cos_summert</th>\n",
       "      <th>sin_fallt</th>\n",
       "      <th>cos_fallt</th>\n",
       "      <th>sin_year_dayt</th>\n",
       "      <th>cos_year_dayt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.518532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956235</td>\n",
       "      <td>0.2926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aep  Is_holiday1  Is_holiday2  Is_Weekend1  Is_Weekend2  sin_month  \\\n",
       "0  0.518532          1.0          0.0          1.0          0.0        1.0   \n",
       "\n",
       "      cos_month  sin_week  cos_week  sin_hour  ...  sin_wintert  cos_wintert  \\\n",
       "0  6.123234e-17  0.866025      -0.5 -0.707107  ...          0.0          1.0   \n",
       "\n",
       "   sin_springt   cos_springt  sin_summert  cos_summert  sin_fallt  cos_fallt  \\\n",
       "0          1.0  6.123234e-17          0.0          1.0        0.0        1.0   \n",
       "\n",
       "   sin_year_dayt  cos_year_dayt  \n",
       "0       0.956235         0.2926  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_te.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.18532385e-01,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "        0.00000000e+00,  1.00000000e+00,  6.12323400e-17,  8.66025404e-01,\n",
       "       -5.00000000e-01, -7.07106781e-01, -7.07106781e-01,  0.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00,  6.12323400e-17,  0.00000000e+00,\n",
       "        1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  9.56234827e-01,\n",
       "        2.92600336e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=2\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 24        # past 24 hours used for prediction (example)\n",
    "target_len = 12        # predict next 12 hours (example)\n",
    "\n",
    "train_X , train_y = univariate_multi_step(train_set, time_steps, target_col=0, target_len=target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.69100212e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         2.58819045e-01,  9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.41395233e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         5.00000000e-01,  8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.26232372e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         7.07106781e-01,  7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.20304505e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         8.66025404e-01,  5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.25545988e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         9.65925826e-01,  2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.62922751e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         1.00000000e+00,  6.12323400e-17,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 2.51029577e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         9.65925826e-01, -2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.08810683e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         8.66025404e-01, -5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.26594284e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         7.07106781e-01, -7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.40009984e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         5.00000000e-01, -8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.47747410e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         2.58819045e-01, -9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.56046425e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         1.22464680e-16, -1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.57855984e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -2.58819045e-01, -9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.73518033e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -5.00000000e-01, -8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.78759516e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -7.07106781e-01, -7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.78759516e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -8.66025404e-01, -5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.72831649e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -9.65925826e-01, -2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.54673655e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -1.00000000e+00, -1.83697020e-16,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.34768501e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -9.65925826e-01,  2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.45813054e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -8.66025404e-01,  5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.54424061e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -7.07106781e-01,  7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.22475977e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -5.00000000e-01,  8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 2.74429053e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -2.58819045e-01,  9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 2.17022339e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01,  5.00000000e-01,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99250011e-01,  3.87222809e-02]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16167478, 0.1249844 , 0.10501685, 0.09409709, 0.09191314,\n",
       "       0.10370648, 0.13708973, 0.1695994 , 0.21683514, 0.25227755,\n",
       "       0.27536503, 0.27524023])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps =24\n",
    "train_X , train_y = univariate_multi_step(train_set, time_steps, target_col=1,target_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.69100212e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         2.58819045e-01,  9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.41395233e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         5.00000000e-01,  8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.26232372e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         7.07106781e-01,  7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.20304505e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         8.66025404e-01,  5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.25545988e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         9.65925826e-01,  2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.62922751e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         1.00000000e+00,  6.12323400e-17,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 2.51029577e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         9.65925826e-01, -2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.08810683e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         8.66025404e-01, -5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.26594284e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         7.07106781e-01, -7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.40009984e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         5.00000000e-01, -8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.47747410e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         2.58819045e-01, -9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.56046425e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         1.22464680e-16, -1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.57855984e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -2.58819045e-01, -9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.73518033e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -5.00000000e-01, -8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.78759516e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -7.07106781e-01, -7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.78759516e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -8.66025404e-01, -5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.72831649e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -9.65925826e-01, -2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.54673655e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -1.00000000e+00, -1.83697020e-16,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.34768501e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -9.65925826e-01,  2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.45813054e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -8.66025404e-01,  5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.54424061e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -7.07106781e-01,  7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 3.22475977e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -5.00000000e-01,  8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 2.74429053e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -2.58819045e-01,  9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 2.17022339e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01,  5.00000000e-01,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99250011e-01,  3.87222809e-02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.5080997943878174 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X , train_y = univariate_multi_step(train_set, time_steps, target_col=0,target_len=1)\n",
    "validation_X, validation_y = univariate_multi_step(validation_set, time_steps, target_col=0,target_len=1)\n",
    "test_X, test_y = univariate_multi_step(test_set, time_steps, target_col=0,target_len=1)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m2638/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0643 - mae: 0.0643 - mape: 73.3850\n",
      "Epoch 1: val_loss improved from inf to 0.02677, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.0642 - mae: 0.0642 - mape: 76.6993 - val_loss: 0.0268 - val_mae: 0.0268 - val_mape: 11.3731\n",
      "Epoch 2/60\n",
      "\u001b[1m2627/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0259 - mae: 0.0259 - mape: 1256.8370\n",
      "Epoch 2: val_loss improved from 0.02677 to 0.02198, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0259 - mae: 0.0259 - mape: 1251.7329 - val_loss: 0.0220 - val_mae: 0.0220 - val_mape: 9.0617\n",
      "Epoch 3/60\n",
      "\u001b[1m2644/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0222 - mae: 0.0222 - mape: 21.5166\n",
      "Epoch 3: val_loss improved from 0.02198 to 0.02018, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0222 - mae: 0.0222 - mape: 21.7860 - val_loss: 0.0202 - val_mae: 0.0202 - val_mape: 8.3428\n",
      "Epoch 4/60\n",
      "\u001b[1m2640/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.0198 - mape: 117.5176\n",
      "Epoch 4: val_loss improved from 0.02018 to 0.01748, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.0198 - mape: 117.8982 - val_loss: 0.0175 - val_mae: 0.0175 - val_mape: 7.5882\n",
      "Epoch 5/60\n",
      "\u001b[1m2628/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0184 - mae: 0.0184 - mape: 227.7525\n",
      "Epoch 5: val_loss did not improve from 0.01748\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0184 - mae: 0.0184 - mape: 226.3911 - val_loss: 0.0187 - val_mae: 0.0187 - val_mape: 8.1097\n",
      "Epoch 6/60\n",
      "\u001b[1m2643/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0172 - mae: 0.0172 - mape: 64.1146\n",
      "Epoch 6: val_loss improved from 0.01748 to 0.01614, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0172 - mae: 0.0172 - mape: 64.5506 - val_loss: 0.0161 - val_mae: 0.0161 - val_mape: 6.9912\n",
      "Epoch 7/60\n",
      "\u001b[1m2632/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0165 - mae: 0.0165 - mape: 626.5804\n",
      "Epoch 7: val_loss improved from 0.01614 to 0.01349, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0165 - mae: 0.0165 - mape: 622.8133 - val_loss: 0.0135 - val_mae: 0.0135 - val_mape: 6.2795\n",
      "Epoch 8/60\n",
      "\u001b[1m2642/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0161 - mae: 0.0161 - mape: 72.2269\n",
      "Epoch 8: val_loss did not improve from 0.01349\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0161 - mae: 0.0161 - mape: 73.8476 - val_loss: 0.0152 - val_mae: 0.0152 - val_mape: 6.3645\n",
      "Epoch 9/60\n",
      "\u001b[1m2633/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0155 - mae: 0.0155 - mape: 21.1958\n",
      "Epoch 9: val_loss did not improve from 0.01349\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0155 - mae: 0.0155 - mape: 22.9706 - val_loss: 0.0204 - val_mae: 0.0204 - val_mape: 8.2687\n",
      "Epoch 10/60\n",
      "\u001b[1m2634/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0152 - mae: 0.0152 - mape: 91.3524\n",
      "Epoch 10: val_loss did not improve from 0.01349\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0152 - mae: 0.0152 - mape: 91.9944 - val_loss: 0.0165 - val_mae: 0.0165 - val_mape: 7.2768\n",
      "Epoch 11/60\n",
      "\u001b[1m2638/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0148 - mae: 0.0148 - mape: 74.3765\n",
      "Epoch 11: val_loss did not improve from 0.01349\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0148 - mae: 0.0148 - mape: 74.8694 - val_loss: 0.0147 - val_mae: 0.0147 - val_mape: 6.4352\n",
      "Epoch 12/60\n",
      "\u001b[1m2634/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0144 - mae: 0.0144 - mape: 145.5150\n",
      "Epoch 12: val_loss did not improve from 0.01349\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0144 - mae: 0.0144 - mape: 147.8927 - val_loss: 0.0171 - val_mae: 0.0171 - val_mape: 7.0420\n",
      "Epoch 13/60\n",
      "\u001b[1m2638/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0143 - mae: 0.0143 - mape: 336.6858\n",
      "Epoch 13: val_loss improved from 0.01349 to 0.01327, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0143 - mae: 0.0143 - mape: 335.6502 - val_loss: 0.0133 - val_mae: 0.0133 - val_mape: 5.7678\n",
      "Epoch 14/60\n",
      "\u001b[1m2630/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0140 - mae: 0.0140 - mape: 126.5931\n",
      "Epoch 14: val_loss did not improve from 0.01327\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0140 - mae: 0.0140 - mape: 126.5587 - val_loss: 0.0135 - val_mae: 0.0135 - val_mape: 5.5085\n",
      "Epoch 15/60\n",
      "\u001b[1m2625/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0136 - mae: 0.0136 - mape: 628.1799\n",
      "Epoch 15: val_loss did not improve from 0.01327\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0136 - mae: 0.0136 - mape: 624.4377 - val_loss: 0.0156 - val_mae: 0.0156 - val_mape: 7.2148\n",
      "Epoch 16/60\n",
      "\u001b[1m2626/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0136 - mae: 0.0136 - mape: 48.9112\n",
      "Epoch 16: val_loss did not improve from 0.01327\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0136 - mae: 0.0136 - mape: 51.0252 - val_loss: 0.0146 - val_mae: 0.0146 - val_mape: 6.7065\n",
      "Epoch 17/60\n",
      "\u001b[1m2642/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0135 - mae: 0.0135 - mape: 11.5286\n",
      "Epoch 17: val_loss improved from 0.01327 to 0.01304, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0135 - mae: 0.0135 - mape: 12.4132 - val_loss: 0.0130 - val_mae: 0.0130 - val_mape: 6.0751\n",
      "Epoch 18/60\n",
      "\u001b[1m2627/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0130 - mae: 0.0130 - mape: 550.3243\n",
      "Epoch 18: val_loss improved from 0.01304 to 0.01236, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0130 - mae: 0.0130 - mape: 548.6199 - val_loss: 0.0124 - val_mae: 0.0124 - val_mape: 5.7326\n",
      "Epoch 19/60\n",
      "\u001b[1m2650/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0130 - mae: 0.0130 - mape: 626.2645\n",
      "Epoch 19: val_loss improved from 0.01236 to 0.01186, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0130 - mae: 0.0130 - mape: 625.9924 - val_loss: 0.0119 - val_mae: 0.0119 - val_mape: 5.8091\n",
      "Epoch 20/60\n",
      "\u001b[1m2650/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0127 - mae: 0.0127 - mape: 372.5752\n",
      "Epoch 20: val_loss improved from 0.01186 to 0.01134, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0127 - mae: 0.0127 - mape: 372.2713 - val_loss: 0.0113 - val_mae: 0.0113 - val_mape: 5.4048\n",
      "Epoch 21/60\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0126 - mae: 0.0126 - mape: 336.3493\n",
      "Epoch 21: val_loss did not improve from 0.01134\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0126 - mae: 0.0126 - mape: 336.3730 - val_loss: 0.0119 - val_mae: 0.0119 - val_mape: 5.2219\n",
      "Epoch 22/60\n",
      "\u001b[1m2648/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0124 - mae: 0.0124 - mape: 390.8401\n",
      "Epoch 22: val_loss did not improve from 0.01134\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0124 - mae: 0.0124 - mape: 390.4378 - val_loss: 0.0152 - val_mae: 0.0152 - val_mape: 7.4210\n",
      "Epoch 23/60\n",
      "\u001b[1m2652/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0125 - mae: 0.0125 - mape: 24.8144\n",
      "Epoch 23: val_loss did not improve from 0.01134\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.0125 - mae: 0.0125 - mape: 25.1662 - val_loss: 0.0121 - val_mae: 0.0121 - val_mape: 5.2928\n",
      "Epoch 24/60\n",
      "\u001b[1m2639/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0125 - mae: 0.0125 - mape: 19.4599\n",
      "Epoch 24: val_loss did not improve from 0.01134\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0125 - mae: 0.0125 - mape: 22.7955 - val_loss: 0.0118 - val_mae: 0.0118 - val_mape: 5.5432\n",
      "Epoch 25/60\n",
      "\u001b[1m2648/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0123 - mae: 0.0123 - mape: 650.1212\n",
      "Epoch 25: val_loss did not improve from 0.01134\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0123 - mae: 0.0123 - mape: 649.9596 - val_loss: 0.0125 - val_mae: 0.0125 - val_mape: 6.4498\n",
      "Epoch 26/60\n",
      "\u001b[1m2652/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0120 - mae: 0.0120 - mape: 337.4759\n",
      "Epoch 26: val_loss did not improve from 0.01134\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0120 - mae: 0.0120 - mape: 337.4016 - val_loss: 0.0116 - val_mae: 0.0116 - val_mape: 5.5288\n",
      "Epoch 27/60\n",
      "\u001b[1m2643/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0122 - mae: 0.0122 - mape: 291.0900\n",
      "Epoch 27: val_loss did not improve from 0.01134\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.0122 - mae: 0.0122 - mape: 291.2806 - val_loss: 0.0121 - val_mae: 0.0121 - val_mape: 6.0826\n",
      "Epoch 28/60\n",
      "\u001b[1m2643/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0122 - mae: 0.0122 - mape: 170.7673\n",
      "Epoch 28: val_loss did not improve from 0.01134\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0122 - mae: 0.0122 - mape: 171.1752 - val_loss: 0.0138 - val_mae: 0.0138 - val_mape: 8.1349\n",
      "Epoch 29/60\n",
      "\u001b[1m2650/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0121 - mae: 0.0121 - mape: 403.7786\n",
      "Epoch 29: val_loss improved from 0.01134 to 0.01072, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0121 - mae: 0.0121 - mape: 403.5578 - val_loss: 0.0107 - val_mae: 0.0107 - val_mape: 4.8888\n",
      "Epoch 30/60\n",
      "\u001b[1m2640/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0119 - mae: 0.0119 - mape: 193.2371\n",
      "Epoch 30: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.0119 - mae: 0.0119 - mape: 193.7893 - val_loss: 0.0121 - val_mae: 0.0121 - val_mape: 6.5523\n",
      "Epoch 31/60\n",
      "\u001b[1m2647/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0119 - mae: 0.0119 - mape: 84.7131\n",
      "Epoch 31: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0119 - mae: 0.0119 - mape: 85.0108 - val_loss: 0.0139 - val_mae: 0.0139 - val_mape: 6.3233\n",
      "Epoch 32/60\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0120 - mae: 0.0120 - mape: 49.5558\n",
      "Epoch 32: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0120 - mae: 0.0120 - mape: 49.6339 - val_loss: 0.0119 - val_mae: 0.0119 - val_mape: 5.4738\n",
      "Epoch 33/60\n",
      "\u001b[1m2651/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0119 - mae: 0.0119 - mape: 43.0116\n",
      "Epoch 33: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0119 - mae: 0.0119 - mape: 43.1835 - val_loss: 0.0108 - val_mae: 0.0108 - val_mape: 5.1047\n",
      "Epoch 34/60\n",
      "\u001b[1m2635/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0117 - mae: 0.0117 - mape: 119.1415\n",
      "Epoch 34: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0117 - mae: 0.0117 - mape: 119.8685 - val_loss: 0.0121 - val_mae: 0.0121 - val_mape: 5.9067\n",
      "Epoch 35/60\n",
      "\u001b[1m2648/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0118 - mae: 0.0118 - mape: 245.0464\n",
      "Epoch 35: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0118 - mae: 0.0118 - mape: 245.1623 - val_loss: 0.0143 - val_mae: 0.0143 - val_mape: 6.5028\n",
      "Epoch 36/60\n",
      "\u001b[1m2630/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0116 - mae: 0.0116 - mape: 447.3201\n",
      "Epoch 36: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0116 - mae: 0.0116 - mape: 445.0588 - val_loss: 0.0124 - val_mae: 0.0124 - val_mape: 6.2769\n",
      "Epoch 37/60\n",
      "\u001b[1m2626/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0116 - mae: 0.0116 - mape: 46.3730\n",
      "Epoch 37: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0116 - mae: 0.0116 - mape: 48.3209 - val_loss: 0.0118 - val_mae: 0.0118 - val_mape: 6.1204\n",
      "Epoch 38/60\n",
      "\u001b[1m2647/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0115 - mae: 0.0115 - mape: 17.4044\n",
      "Epoch 38: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0115 - mae: 0.0115 - mape: 18.4055 - val_loss: 0.0122 - val_mae: 0.0122 - val_mape: 5.6789\n",
      "Epoch 39/60\n",
      "\u001b[1m2652/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0116 - mae: 0.0116 - mape: 66.6795\n",
      "Epoch 39: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0116 - mae: 0.0116 - mape: 66.6999 - val_loss: 0.0125 - val_mae: 0.0125 - val_mape: 5.7725\n",
      "Epoch 40/60\n",
      "\u001b[1m2633/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0114 - mae: 0.0114 - mape: 195.4041\n",
      "Epoch 40: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0114 - mae: 0.0114 - mape: 196.7015 - val_loss: 0.0116 - val_mae: 0.0116 - val_mape: 5.6338\n",
      "Epoch 41/60\n",
      "\u001b[1m2627/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0115 - mae: 0.0115 - mape: 4.7382\n",
      "Epoch 41: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0115 - mae: 0.0115 - mape: 6.5084 - val_loss: 0.0116 - val_mae: 0.0116 - val_mape: 5.5009\n",
      "Epoch 42/60\n",
      "\u001b[1m2630/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0114 - mae: 0.0114 - mape: 29.0586\n",
      "Epoch 42: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.0114 - mae: 0.0114 - mape: 30.6508 - val_loss: 0.0115 - val_mae: 0.0115 - val_mape: 5.6393\n",
      "Epoch 43/60\n",
      "\u001b[1m2641/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0114 - mae: 0.0114 - mape: 30.8979\n",
      "Epoch 43: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0114 - mae: 0.0114 - mape: 32.3885 - val_loss: 0.0119 - val_mae: 0.0119 - val_mape: 5.5074\n",
      "Epoch 44/60\n",
      "\u001b[1m2652/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0114 - mae: 0.0114 - mape: 227.8101\n",
      "Epoch 44: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0114 - mae: 0.0114 - mape: 227.8733 - val_loss: 0.0117 - val_mae: 0.0117 - val_mape: 5.9077\n",
      "Epoch 45/60\n",
      "\u001b[1m2631/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0113 - mae: 0.0113 - mape: 86.6504\n",
      "Epoch 45: val_loss did not improve from 0.01072\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0113 - mae: 0.0113 - mape: 87.9212 - val_loss: 0.0120 - val_mae: 0.0120 - val_mape: 5.7468\n",
      "Epoch 46/60\n",
      "\u001b[1m2650/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 - mae: 0.0112 - mape: 47.4275\n",
      "Epoch 46: val_loss improved from 0.01072 to 0.01015, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0112 - mae: 0.0112 - mape: 47.8953 - val_loss: 0.0101 - val_mae: 0.0101 - val_mape: 4.9062\n",
      "Epoch 47/60\n",
      "\u001b[1m2632/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 - mae: 0.0112 - mape: 91.6095\n",
      "Epoch 47: val_loss did not improve from 0.01015\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0112 - mae: 0.0112 - mape: 93.5157 - val_loss: 0.0117 - val_mae: 0.0117 - val_mape: 5.6246\n",
      "Epoch 48/60\n",
      "\u001b[1m2642/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0113 - mae: 0.0113 - mape: 32.8878\n",
      "Epoch 48: val_loss did not improve from 0.01015\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0113 - mae: 0.0113 - mape: 33.5722 - val_loss: 0.0123 - val_mae: 0.0123 - val_mape: 6.1050\n",
      "Epoch 49/60\n",
      "\u001b[1m2633/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0113 - mae: 0.0113 - mape: 45.4269\n",
      "Epoch 49: val_loss did not improve from 0.01015\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0113 - mae: 0.0113 - mape: 46.8643 - val_loss: 0.0111 - val_mae: 0.0111 - val_mape: 5.1260\n",
      "Epoch 50/60\n",
      "\u001b[1m2647/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0113 - mae: 0.0113 - mape: 640.1762\n",
      "Epoch 50: val_loss did not improve from 0.01015\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0113 - mae: 0.0113 - mape: 639.3546 - val_loss: 0.0110 - val_mae: 0.0110 - val_mape: 5.0133\n",
      "Epoch 51/60\n",
      "\u001b[1m2637/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0111 - mae: 0.0111 - mape: 134.9082\n",
      "Epoch 51: val_loss did not improve from 0.01015\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0111 - mae: 0.0111 - mape: 135.9681 - val_loss: 0.0127 - val_mae: 0.0127 - val_mape: 6.2111\n",
      "Epoch 52/60\n",
      "\u001b[1m2649/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 - mae: 0.0112 - mape: 316.5476\n",
      "Epoch 52: val_loss improved from 0.01015 to 0.01014, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0112 - mae: 0.0112 - mape: 316.6672 - val_loss: 0.0101 - val_mae: 0.0101 - val_mape: 5.0359\n",
      "Epoch 53/60\n",
      "\u001b[1m2647/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0109 - mae: 0.0109 - mape: 369.1542\n",
      "Epoch 53: val_loss did not improve from 0.01014\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0109 - mae: 0.0109 - mape: 368.7343 - val_loss: 0.0115 - val_mae: 0.0115 - val_mape: 5.3184\n",
      "Epoch 54/60\n",
      "\u001b[1m2643/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0111 - mae: 0.0111 - mape: 18.3989\n",
      "Epoch 54: val_loss improved from 0.01014 to 0.01001, saving model to D:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0111 - mae: 0.0111 - mape: 19.6524 - val_loss: 0.0100 - val_mae: 0.0100 - val_mape: 4.7552\n",
      "Epoch 55/60\n",
      "\u001b[1m2631/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0109 - mae: 0.0109 - mape: 63.2503\n",
      "Epoch 55: val_loss did not improve from 0.01001\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0109 - mae: 0.0109 - mape: 64.7978 - val_loss: 0.0114 - val_mae: 0.0114 - val_mape: 5.5031\n",
      "Epoch 56/60\n",
      "\u001b[1m2650/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0110 - mae: 0.0110 - mape: 56.0165\n",
      "Epoch 56: val_loss did not improve from 0.01001\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0110 - mae: 0.0110 - mape: 56.1073 - val_loss: 0.0123 - val_mae: 0.0123 - val_mape: 5.6122\n",
      "Epoch 57/60\n",
      "\u001b[1m2652/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0110 - mae: 0.0110 - mape: 10.7187\n",
      "Epoch 57: val_loss did not improve from 0.01001\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0110 - mae: 0.0110 - mape: 10.8422 - val_loss: 0.0121 - val_mae: 0.0121 - val_mape: 5.8810\n",
      "Epoch 58/60\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0110 - mae: 0.0110 - mape: 19.7693\n",
      "Epoch 58: val_loss did not improve from 0.01001\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0110 - mae: 0.0110 - mape: 19.7668 - val_loss: 0.0112 - val_mae: 0.0112 - val_mape: 5.2172\n",
      "Epoch 59/60\n",
      "\u001b[1m2647/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0108 - mae: 0.0108 - mape: 248.2704\n",
      "Epoch 59: val_loss did not improve from 0.01001\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0108 - mae: 0.0108 - mape: 247.9645 - val_loss: 0.0102 - val_mae: 0.0102 - val_mape: 4.7502\n",
      "Epoch 60/60\n",
      "\u001b[1m2648/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 - mae: 0.0112 - mape: 305.2464\n",
      "Epoch 60: val_loss did not improve from 0.01001\n",
      "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0112 - mae: 0.0112 - mape: 305.0961 - val_loss: 0.0109 - val_mae: 0.0109 - val_mape: 5.3676\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                    verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step\n",
      "Mean Absolute Error (MAE): 166.31\n",
      "Median Absolute Error (MedAE): 135.43\n",
      "Mean Squared Error (MSE): 45648.07\n",
      "Root Mean Squared Error (RMSE): 213.65\n",
      "Mean Absolute Percentage Error (MAPE): 1.14 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.94 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(r'd:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras')\n",
    "\n",
    "y_pred_scaled   = model.predict(test_X)\n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled)) \n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = r'd:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras'\n",
    "model=r'd:\\ML LABS\\lab 8,9,10\\chkcnn\\E1-cp-0001-val_loss0.02.keras'\n",
    "start_epoch= 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model from: <Sequential name=sequential_1, built=True>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_path_isdir: path should be string, bytes, os.PathLike or integer, not Sequential",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] loading model from: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model))\n\u001b[1;32m---> 17\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model(model)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] old learning rate: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(K\u001b[38;5;241m.\u001b[39mget_value(model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mlearning_rate)))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:164\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads a model saved via `model.save()`.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03maccess specific variables, e.g. `model.get_layer(\"dense_1\").kernel`.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mis_zipfile(\n\u001b[0;32m    162\u001b[0m     filepath\n\u001b[0;32m    163\u001b[0m )\n\u001b[1;32m--> 164\u001b[0m is_keras_dir \u001b[38;5;241m=\u001b[39m file_utils\u001b[38;5;241m.\u001b[39misdir(filepath) \u001b[38;5;129;01mand\u001b[39;00m file_utils\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    165\u001b[0m     file_utils\u001b[38;5;241m.\u001b[39mjoin(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m )\n\u001b[0;32m    167\u001b[0m is_hf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf://\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Support for remote zip files\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\file_utils.py:476\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m         _raise_if_no_gfile(path)\n\u001b[1;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path)\n",
      "\u001b[1;31mTypeError\u001b[0m: _path_isdir: path should be string, bytes, os.PathLike or integer, not Sequential"
     ]
    }
   ],
   "source": [
    "# construct the callback to save only the *best* model to disk based on val_loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                                   monitor=\"val_loss\",\n",
    "                                   save_best_only=True, \n",
    "                                   verbose=1)\n",
    "\n",
    "callbacks = [EpochCheckpoint1]\n",
    "\n",
    "# if there is no specific model checkpoint supplied, then initialize and compile\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=24, num_features=21, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss='mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "else:\n",
    "    print(\"[INFO] loading model from: {}\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.learning_rate)))\n",
    "    K.set_value(model.optimizer.learning_rate, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.learning_rate)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2625/2653 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0091 - mape: 94.3988\n",
      "Epoch 1: val_loss improved from inf to 0.00950, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab2\\E2-cp-0001-loss0.01.h5\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0091 - mae: 0.0091 - mape: 93.4461 - val_loss: 0.0095 - val_mae: 0.0095 - val_mape: 4.6159\n",
      "Epoch 2/10\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 0.0090 - mae: 0.0090 - mape: 45.7124\n",
      "Epoch 2: val_loss improved from 0.00950 to 0.00920, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab2\\E2-cp-0002-loss0.01.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0090 - mae: 0.0090 - mape: 45.7124 - val_loss: 0.0092 - val_mae: 0.0092 - val_mape: 4.3533\n",
      "Epoch 3/10\n",
      "2632/2653 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0090 - mape: 53.8044\n",
      "Epoch 3: val_loss did not improve from 0.00920\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0090 - mae: 0.0090 - mape: 53.4095 - val_loss: 0.0095 - val_mae: 0.0095 - val_mape: 4.4109\n",
      "Epoch 4/10\n",
      "2632/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 6.2736\n",
      "Epoch 4: val_loss did not improve from 0.00920\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0090 - mae: 0.0090 - mape: 6.2483 - val_loss: 0.0093 - val_mae: 0.0093 - val_mape: 4.2993\n",
      "Epoch 5/10\n",
      "2646/2653 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0090 - mape: 4.1870\n",
      "Epoch 5: val_loss did not improve from 0.00920\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0090 - mae: 0.0090 - mape: 4.1830 - val_loss: 0.0092 - val_mae: 0.0092 - val_mape: 4.4293\n",
      "Epoch 6/10\n",
      "2631/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 40.0921\n",
      "Epoch 6: val_loss improved from 0.00920 to 0.00898, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab2\\E2-cp-0006-loss0.01.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 39.7903 - val_loss: 0.0090 - val_mae: 0.0090 - val_mape: 4.0811\n",
      "Epoch 7/10\n",
      "2628/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 51.3619\n",
      "Epoch 7: val_loss did not improve from 0.00898\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 50.9114 - val_loss: 0.0092 - val_mae: 0.0092 - val_mape: 4.2066\n",
      "Epoch 8/10\n",
      "2643/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 54.4526\n",
      "Epoch 8: val_loss did not improve from 0.00898\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 54.2669 - val_loss: 0.0096 - val_mae: 0.0096 - val_mape: 4.3860\n",
      "Epoch 9/10\n",
      "2638/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 35.8424\n",
      "Epoch 9: val_loss did not improve from 0.00898\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 35.6599 - val_loss: 0.0091 - val_mae: 0.0091 - val_mape: 4.1363\n",
      "Epoch 10/10\n",
      "2628/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 29.1699\n",
      "Epoch 10: val_loss did not improve from 0.00898\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 28.9249 - val_loss: 0.0096 - val_mae: 0.0096 - val_mape: 4.3660\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 1s 1ms/step\n",
      "Mean Absolute Error (MAE): 144.75\n",
      "Median Absolute Error (MedAE): 113.65\n",
      "Mean Squared Error (MSE): 36189.68\n",
      "Root Mean Squared Error (RMSE): 190.24\n",
      "Mean Absolute Percentage Error (MAPE): 0.98 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.79 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(r'C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E2-cp-0006-loss0.01.h5')\n",
    "\n",
    "y_pred_scaled   = model.predict(test_X)\n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled)) \n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
